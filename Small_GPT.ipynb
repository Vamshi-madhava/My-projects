{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFkHGgkeVU9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQCU00ZXVUyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BmVjHUMVUha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF7pO-nXy1iv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from transformers import GPT2TokenizerFast\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def stream_tokenize_and_save_safe(input_path, output_path, chunk_size=500000):\n",
        "    print(f\"Tokenizing: {input_path}\")\n",
        "    token_buffer = []\n",
        "\n",
        "    total_tokens = 0\n",
        "    chunk_idx = 0\n",
        "\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f, open(output_path, \"wb\") as out_bin:\n",
        "        lines = []\n",
        "        for i, line in enumerate(tqdm(f, desc=\"Reading lines\")):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                ids = tokenizer.encode(line)\n",
        "                token_buffer.extend(ids + [tokenizer.eos_token_id])\n",
        "                total_tokens += len(ids) + 1\n",
        "\n",
        "            if (i + 1) % chunk_size == 0:\n",
        "                np.array(token_buffer, dtype=np.uint16).tofile(out_bin)\n",
        "                token_buffer.clear()\n",
        "                chunk_idx += 1\n",
        "                print(f\"Wrote chunk {chunk_idx * chunk_size} lines, total tokens: {total_tokens:,}\")\n",
        "\n",
        "        # Write remaining tokens\n",
        "        if token_buffer:\n",
        "            np.array(token_buffer, dtype=np.uint16).tofile(out_bin)\n",
        "            print(f\" Wrote final chunk, total tokens: {total_tokens:,}\")\n",
        "\n",
        "    print(f\"\\n Final token count: {total_tokens:,}\")\n",
        "    print(f\"Saved to {output_path}\\n\")\n",
        "\n",
        "\n",
        "stream_tokenize_and_save_safe(\"/content/drive/MyDrive/minipile_train.txt\", \"/content/drive/MyDrive/train.bin\")\n",
        "stream_tokenize_and_save_safe(\"/content/drive/MyDrive/minipile_val.txt\", \"/content/drive/MyDrive/val.bin\")\n",
        "stream_tokenize_and_save_safe(\"/content/drive/MyDrive/minipile_test.txt\", \"/content/drive/MyDrive/test.bin\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "65GRU3RiNz4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IeZsMOpPr-P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gAmaA6eYy1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eed91098-d36d-4938-f900-4bf589b1ecc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-1614383758.py:153: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-1-1614383758.py:165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 10.9899\n",
            "Step 50, Loss: 6.3487\n",
            "Step 100, Loss: 5.8830\n",
            "Step 150, Loss: 5.8285\n",
            "Step 200, Loss: 5.6711\n",
            "Step 250, Loss: 5.6177\n",
            "Step 300, Loss: 5.5895\n",
            "Step 350, Loss: 5.2921\n",
            "Step 400, Loss: 5.4705\n",
            "Step 450, Loss: 5.2567\n",
            "Step 500, Loss: 5.2369\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 550, Loss: 5.3903\n",
            "Step 600, Loss: 5.2548\n",
            "Step 650, Loss: 5.0381\n",
            "Step 700, Loss: 4.9939\n",
            "Step 750, Loss: 4.9524\n",
            "Step 800, Loss: 4.8803\n",
            "Step 850, Loss: 4.9127\n",
            "Step 900, Loss: 4.7042\n",
            "Step 950, Loss: 4.7565\n",
            "Step 1000, Loss: 4.9229\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 1050, Loss: 4.8421\n",
            "Step 1100, Loss: 4.7776\n",
            "Step 1150, Loss: 4.6591\n",
            "Step 1200, Loss: 4.7500\n",
            "Step 1250, Loss: 4.8809\n",
            "Step 1300, Loss: 4.6579\n",
            "Step 1350, Loss: 4.7335\n",
            "Step 1400, Loss: 4.6339\n",
            "Step 1450, Loss: 4.5815\n",
            "Step 1500, Loss: 4.5888\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 1550, Loss: 4.5352\n",
            "Step 1600, Loss: 4.6045\n",
            "Step 1650, Loss: 4.5963\n",
            "Step 1700, Loss: 4.5090\n",
            "Step 1750, Loss: 4.4087\n",
            "Step 1800, Loss: 4.6013\n",
            "Step 1850, Loss: 4.5222\n",
            "Step 1900, Loss: 4.4962\n",
            "Step 1950, Loss: 4.5223\n",
            "Step 2000, Loss: 4.4510\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 2050, Loss: 4.4791\n",
            "Step 2100, Loss: 4.3460\n",
            "Step 2150, Loss: 4.5029\n",
            "Step 2200, Loss: 4.3913\n",
            "Step 2250, Loss: 4.3143\n",
            "Step 2300, Loss: 4.3404\n",
            "Step 2350, Loss: 4.3683\n",
            "Step 2400, Loss: 4.1824\n",
            "Step 2450, Loss: 4.4053\n",
            "Step 2500, Loss: 4.2511\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 2550, Loss: 4.1571\n",
            "Step 2600, Loss: 4.3173\n",
            "Step 2650, Loss: 4.2513\n",
            "Step 2700, Loss: 4.1665\n",
            "Step 2750, Loss: 4.2954\n",
            "Step 2800, Loss: 4.2582\n",
            "Step 2850, Loss: 4.1272\n",
            "Step 2900, Loss: 4.2636\n",
            "Step 2950, Loss: 4.1643\n",
            "Step 3000, Loss: 4.1312\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 3050, Loss: 4.1861\n",
            "Step 3100, Loss: 4.0969\n",
            "Step 3150, Loss: 4.2642\n",
            "Step 3200, Loss: 4.0619\n",
            "Step 3250, Loss: 4.2172\n",
            "Step 3300, Loss: 3.9789\n",
            "Step 3350, Loss: 4.3581\n",
            "Step 3400, Loss: 4.1711\n",
            "Step 3450, Loss: 4.1766\n",
            "Step 3500, Loss: 4.0734\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 3550, Loss: 4.0297\n",
            "Step 3600, Loss: 4.0411\n",
            "Step 3650, Loss: 3.8866\n",
            "Step 3700, Loss: 4.1309\n",
            "Step 3750, Loss: 4.2160\n",
            "Step 3800, Loss: 4.2644\n",
            "Step 3850, Loss: 4.0589\n",
            "Step 3900, Loss: 4.1124\n",
            "Step 3950, Loss: 4.0382\n",
            "Step 4000, Loss: 4.0915\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 4050, Loss: 4.0230\n",
            "Step 4100, Loss: 3.9575\n",
            "Step 4150, Loss: 4.0859\n",
            "Step 4200, Loss: 3.9143\n",
            "Step 4250, Loss: 3.9444\n",
            "Step 4300, Loss: 3.9104\n",
            "Step 4350, Loss: 3.9329\n",
            "Step 4400, Loss: 3.9564\n",
            "Step 4450, Loss: 4.1179\n",
            "Step 4500, Loss: 4.0084\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 4550, Loss: 4.0274\n",
            "Step 4600, Loss: 3.9843\n",
            "Step 4650, Loss: 3.9658\n",
            "Step 4700, Loss: 3.9150\n",
            "Step 4750, Loss: 3.9581\n",
            "Step 4800, Loss: 3.9967\n",
            "Step 4850, Loss: 4.0688\n",
            "Step 4900, Loss: 3.9194\n",
            "Step 4950, Loss: 3.8536\n",
            "Step 5000, Loss: 3.9249\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 5050, Loss: 3.9925\n",
            "Step 5100, Loss: 4.0231\n",
            "Step 5150, Loss: 3.8525\n",
            "Step 5200, Loss: 4.0614\n",
            "Step 5250, Loss: 3.9782\n",
            "Step 5300, Loss: 4.0640\n",
            "Step 5350, Loss: 3.8790\n",
            "Step 5400, Loss: 3.8889\n",
            "Step 5450, Loss: 3.9323\n",
            "Step 5500, Loss: 3.9779\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 5550, Loss: 3.9364\n",
            "Step 5600, Loss: 3.8853\n",
            "Step 5650, Loss: 3.8156\n",
            "Step 5700, Loss: 3.8752\n",
            "Step 5750, Loss: 3.7516\n",
            "Step 5800, Loss: 4.0787\n",
            "Step 5850, Loss: 3.8933\n",
            "Step 5900, Loss: 4.0742\n",
            "Step 5950, Loss: 3.9746\n",
            "Step 6000, Loss: 3.9719\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 6050, Loss: 3.8631\n",
            "Step 6100, Loss: 3.9762\n",
            "Step 6150, Loss: 3.9745\n",
            "Step 6200, Loss: 3.7916\n",
            "Step 6250, Loss: 3.8963\n",
            "Step 6300, Loss: 3.9248\n",
            "Step 6350, Loss: 4.0730\n",
            "Step 6400, Loss: 3.8148\n",
            "Step 6450, Loss: 3.7711\n",
            "Step 6500, Loss: 3.8409\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 6550, Loss: 3.6545\n",
            "Step 6600, Loss: 3.7501\n",
            "Step 6650, Loss: 3.8976\n",
            "Step 6700, Loss: 3.8612\n",
            "Step 6750, Loss: 3.6634\n",
            "Step 6800, Loss: 3.9023\n",
            "Step 6850, Loss: 3.9053\n",
            "Step 6900, Loss: 3.9912\n",
            "Step 6950, Loss: 3.7944\n",
            "Step 7000, Loss: 3.8010\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 7050, Loss: 3.8790\n",
            "Step 7100, Loss: 3.7891\n",
            "Step 7150, Loss: 3.9412\n",
            "Step 7200, Loss: 3.7238\n",
            "Step 7250, Loss: 3.8601\n",
            "Step 7300, Loss: 3.8134\n",
            "Step 7350, Loss: 3.7923\n",
            "Step 7400, Loss: 3.7018\n",
            "Step 7450, Loss: 3.6850\n",
            "Step 7500, Loss: 3.7661\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 7550, Loss: 3.7675\n",
            "Step 7600, Loss: 3.7055\n",
            "Step 7650, Loss: 3.7308\n",
            "Step 7700, Loss: 3.8522\n",
            "Step 7750, Loss: 3.7758\n",
            "Step 7800, Loss: 3.6081\n",
            "Step 7850, Loss: 3.7987\n",
            "Step 7900, Loss: 3.7431\n",
            "Step 7950, Loss: 3.7332\n",
            "Step 8000, Loss: 3.6522\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 8050, Loss: 3.6595\n",
            "Step 8100, Loss: 3.8215\n",
            "Step 8150, Loss: 3.7944\n",
            "Step 8200, Loss: 3.6267\n",
            "Step 8250, Loss: 3.8793\n",
            "Step 8300, Loss: 3.7093\n",
            "Step 8350, Loss: 3.7981\n",
            "Step 8400, Loss: 3.7709\n",
            "Step 8450, Loss: 3.8300\n",
            "Step 8500, Loss: 3.7976\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 8550, Loss: 3.6460\n",
            "Step 8600, Loss: 3.7236\n",
            "Step 8650, Loss: 3.5800\n",
            "Step 8700, Loss: 3.7759\n",
            "Step 8750, Loss: 3.7981\n",
            "Step 8800, Loss: 3.8856\n",
            "Step 8850, Loss: 3.7222\n",
            "Step 8900, Loss: 3.6549\n",
            "Step 8950, Loss: 3.7702\n",
            "Step 9000, Loss: 3.7319\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 9050, Loss: 3.7848\n",
            "Step 9100, Loss: 3.8574\n",
            "Step 9150, Loss: 3.7769\n",
            "Step 9200, Loss: 3.7661\n",
            "Step 9250, Loss: 3.7928\n",
            "Step 9300, Loss: 3.8052\n",
            "Step 9350, Loss: 3.6846\n",
            "Step 9400, Loss: 3.7840\n",
            "Step 9450, Loss: 3.7512\n",
            "Step 9500, Loss: 3.8207\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 9550, Loss: 3.6698\n",
            "Step 9600, Loss: 3.8609\n",
            "Step 9650, Loss: 3.6588\n",
            "Step 9700, Loss: 3.6459\n",
            "Step 9750, Loss: 3.7190\n",
            "Step 9800, Loss: 3.6943\n",
            "Step 9850, Loss: 3.7963\n",
            "Step 9900, Loss: 3.5821\n",
            "Step 9950, Loss: 3.6629\n",
            "Step 10000, Loss: 3.7038\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 10050, Loss: 3.7214\n",
            "Step 10100, Loss: 3.6308\n",
            "Step 10150, Loss: 3.6642\n",
            "Step 10200, Loss: 3.7280\n",
            "Step 10250, Loss: 3.7764\n",
            "Step 10300, Loss: 3.8316\n",
            "Step 10350, Loss: 3.7677\n",
            "Step 10400, Loss: 3.6738\n",
            "Step 10450, Loss: 3.6507\n",
            "Step 10500, Loss: 3.7180\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 10550, Loss: 3.7640\n",
            "Step 10600, Loss: 3.7412\n",
            "Step 10650, Loss: 3.6440\n",
            "Step 10700, Loss: 3.6792\n",
            "Step 10750, Loss: 3.6933\n",
            "Step 10800, Loss: 3.5955\n",
            "Step 10850, Loss: 3.5858\n",
            "Step 10900, Loss: 3.5056\n",
            "Step 10950, Loss: 3.6589\n",
            "Step 11000, Loss: 3.6556\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 11050, Loss: 3.6067\n",
            "Step 11100, Loss: 3.6459\n",
            "Step 11150, Loss: 3.5562\n",
            "Step 11200, Loss: 3.5466\n",
            "Step 11250, Loss: 3.6248\n",
            "Step 11300, Loss: 3.7849\n",
            "Step 11350, Loss: 3.5882\n",
            "Step 11400, Loss: 3.5591\n",
            "Step 11450, Loss: 3.6119\n",
            "Step 11500, Loss: 3.5996\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 11550, Loss: 3.5252\n",
            "Step 11600, Loss: 3.5782\n",
            "Step 11650, Loss: 3.6920\n",
            "Step 11700, Loss: 3.6395\n",
            "Step 11750, Loss: 3.5506\n",
            "Step 11800, Loss: 3.6324\n",
            "Step 11850, Loss: 3.7523\n",
            "Step 11900, Loss: 3.6394\n",
            "Step 11950, Loss: 3.7038\n",
            "Step 12000, Loss: 3.5842\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 12050, Loss: 3.5923\n",
            "Step 12100, Loss: 3.5059\n",
            "Step 12150, Loss: 3.7704\n",
            "Step 12200, Loss: 3.6810\n",
            "Step 12250, Loss: 3.6507\n",
            "Step 12300, Loss: 3.7261\n",
            "Step 12350, Loss: 3.6730\n",
            "Step 12400, Loss: 3.7204\n",
            "Step 12450, Loss: 3.5522\n",
            "Step 12500, Loss: 3.5959\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 12550, Loss: 3.7285\n",
            "Step 12600, Loss: 3.7080\n",
            "Step 12650, Loss: 3.6343\n",
            "Step 12700, Loss: 3.6498\n",
            "Step 12750, Loss: 3.4586\n",
            "Step 12800, Loss: 3.5849\n",
            "Step 12850, Loss: 3.6033\n",
            "Step 12900, Loss: 3.7353\n",
            "Step 12950, Loss: 3.6317\n",
            "Step 13000, Loss: 3.5470\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 13050, Loss: 3.7077\n",
            "Step 13100, Loss: 3.6107\n",
            "Step 13150, Loss: 3.5072\n",
            "Step 13200, Loss: 3.7267\n",
            "Step 13250, Loss: 3.6813\n",
            "Step 13300, Loss: 3.5239\n",
            "Step 13350, Loss: 3.6047\n",
            "Step 13400, Loss: 3.7077\n",
            "Step 13450, Loss: 3.5776\n",
            "Step 13500, Loss: 3.5415\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 13550, Loss: 3.5880\n",
            "Step 13600, Loss: 3.5435\n",
            "Step 13650, Loss: 3.3942\n",
            "Step 13700, Loss: 3.4974\n",
            "Step 13750, Loss: 3.6672\n",
            "Step 13800, Loss: 3.5760\n",
            "Step 13850, Loss: 3.5886\n",
            "Step 13900, Loss: 3.6028\n",
            "Step 13950, Loss: 3.5055\n",
            "Step 14000, Loss: 3.5704\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 14050, Loss: 3.6608\n",
            "Step 14100, Loss: 3.5308\n",
            "Step 14150, Loss: 3.5611\n",
            "Step 14200, Loss: 3.5025\n",
            "Step 14250, Loss: 3.4481\n",
            "Step 14300, Loss: 3.4259\n",
            "Step 14350, Loss: 3.5186\n",
            "Step 14400, Loss: 3.5602\n",
            "Step 14450, Loss: 3.6012\n",
            "Step 14500, Loss: 3.6814\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 14550, Loss: 3.6501\n",
            "Step 14600, Loss: 3.5867\n",
            "Step 14650, Loss: 3.5184\n",
            "Step 14700, Loss: 3.7114\n",
            "Step 14750, Loss: 3.4116\n",
            "Step 14800, Loss: 3.7570\n",
            "Step 14850, Loss: 3.5630\n",
            "Step 14900, Loss: 3.5389\n",
            "Step 14950, Loss: 3.4030\n",
            "Step 15000, Loss: 3.5765\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 15050, Loss: 3.5729\n",
            "Step 15100, Loss: 3.5671\n",
            "Step 15150, Loss: 3.4255\n",
            "Step 15200, Loss: 3.5828\n",
            "Step 15250, Loss: 3.4782\n",
            "Step 15300, Loss: 3.6291\n",
            "Step 15350, Loss: 3.5816\n",
            "Step 15400, Loss: 3.6235\n",
            "Step 15450, Loss: 3.5992\n",
            "Step 15500, Loss: 3.5709\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 15550, Loss: 3.5977\n",
            "Step 15600, Loss: 3.5049\n",
            "Step 15650, Loss: 3.6343\n",
            "Step 15700, Loss: 3.5599\n",
            "Step 15750, Loss: 3.6085\n",
            "Step 15800, Loss: 3.6021\n",
            "Step 15850, Loss: 3.4976\n",
            "Step 15900, Loss: 3.6123\n",
            "Step 15950, Loss: 3.6744\n",
            "Step 16000, Loss: 3.5731\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 16050, Loss: 3.4566\n",
            "Step 16100, Loss: 3.5487\n",
            "Step 16150, Loss: 3.6842\n",
            "Step 16200, Loss: 3.6272\n",
            "Step 16250, Loss: 3.5775\n",
            "Step 16300, Loss: 3.4271\n",
            "Step 16350, Loss: 3.5957\n",
            "Step 16400, Loss: 3.6231\n",
            "Step 16450, Loss: 3.4604\n",
            "Step 16500, Loss: 3.5301\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 16550, Loss: 3.6340\n",
            "Step 16600, Loss: 3.6235\n",
            "Step 16650, Loss: 3.4479\n",
            "Step 16700, Loss: 3.5883\n",
            "Step 16750, Loss: 3.6961\n",
            "Step 16800, Loss: 3.5662\n",
            "Step 16850, Loss: 3.5700\n",
            "Step 16900, Loss: 3.4845\n",
            "Step 16950, Loss: 3.6103\n",
            "Step 17000, Loss: 3.4528\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 17050, Loss: 3.6487\n",
            "Step 17100, Loss: 3.6356\n",
            "Step 17150, Loss: 3.6535\n",
            "Step 17200, Loss: 3.4873\n",
            "Step 17250, Loss: 3.4996\n",
            "Step 17300, Loss: 3.4268\n",
            "Step 17350, Loss: 3.4639\n",
            "Step 17400, Loss: 3.6429\n",
            "Step 17450, Loss: 3.4620\n",
            "Step 17500, Loss: 3.6109\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 17550, Loss: 3.5936\n",
            "Step 17600, Loss: 3.4745\n",
            "Step 17650, Loss: 3.7024\n",
            "Step 17700, Loss: 3.4835\n",
            "Step 17750, Loss: 3.5593\n",
            "Step 17800, Loss: 3.4989\n",
            "Step 17850, Loss: 3.4860\n",
            "Step 17900, Loss: 3.5544\n",
            "Step 17950, Loss: 3.4712\n",
            "Step 18000, Loss: 3.4553\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 18050, Loss: 3.6309\n",
            "Step 18100, Loss: 3.3769\n",
            "Step 18150, Loss: 3.5066\n",
            "Step 18200, Loss: 3.5392\n",
            "Step 18250, Loss: 3.5715\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1614383758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
        "\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attention = CasualSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 512\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 8\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 512\n",
        "    dropout: float = 0.2\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                token_embedding=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                position_embedding=nn.Embedding(config.block_size, config.n_embd),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        token_embeddings = self.transformer.token_embedding(idx)\n",
        "        position_ids = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        position_embeddings = self.transformer.position_embedding(position_ids)\n",
        "        x = token_embeddings + position_embeddings\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path, block_size):\n",
        "        self.data = np.memmap(path, dtype=np.uint16, mode=\"r\")\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx : idx + self.block_size + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# === Hyperparameters ===\n",
        "block_size = 512\n",
        "batch_size = 32  # increased batch size\n",
        "train_steps = 100_000\n",
        "save_path = \"/content/drive/MyDrive/gpt8x512.pth\"\n",
        "\n",
        "# === Prepare dataset and dataloader ===\n",
        "train_dataset = GPTDataset(\"/content/drive/MyDrive/bookcorpus_tokens.bin\", block_size)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4\n",
        ")\n",
        "\n",
        "# === Model, optimizer, scaler ===\n",
        "config = GPTConfig(block_size=block_size)\n",
        "model = GPT(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# === Training loop ===\n",
        "step = 0\n",
        "while step < train_steps:\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        if step >= train_steps:\n",
        "            break\n",
        "        model.train()\n",
        "        x_batch, y_batch = x_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits, loss = model(x_batch, y_batch)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if step % 500 == 0 and step > 0:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "        step += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}