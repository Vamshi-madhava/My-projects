{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "65GRU3RiNz4S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gAmaA6eYy1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855af61f-46e6-4c30-e700-39dffd6e3914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-2860818213.py:153: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-2-2860818213.py:165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 10.9876\n",
            "Step 50, Loss: 7.0416\n",
            "Step 100, Loss: 6.7561\n",
            "Step 150, Loss: 6.5065\n",
            "Step 200, Loss: 6.3063\n",
            "Step 250, Loss: 6.1529\n",
            "Step 300, Loss: 5.9462\n",
            "Step 350, Loss: 5.9286\n",
            "Step 400, Loss: 5.7024\n",
            "Step 450, Loss: 5.5395\n",
            "Step 500, Loss: 5.5848\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 550, Loss: 5.3420\n",
            "Step 600, Loss: 5.4061\n",
            "Step 650, Loss: 5.2887\n",
            "Step 700, Loss: 5.1824\n",
            "Step 750, Loss: 5.1222\n",
            "Step 800, Loss: 5.0516\n",
            "Step 850, Loss: 5.0232\n",
            "Step 900, Loss: 4.9222\n",
            "Step 950, Loss: 4.9219\n",
            "Step 1000, Loss: 4.8307\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 1050, Loss: 4.7819\n",
            "Step 1100, Loss: 4.7508\n",
            "Step 1150, Loss: 4.7529\n",
            "Step 1200, Loss: 4.6111\n",
            "Step 1250, Loss: 4.6446\n",
            "Step 1300, Loss: 4.6869\n",
            "Step 1350, Loss: 4.6487\n",
            "Step 1400, Loss: 4.5612\n",
            "Step 1450, Loss: 4.5214\n",
            "Step 1500, Loss: 4.4933\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 1550, Loss: 4.4461\n",
            "Step 1600, Loss: 4.4156\n",
            "Step 1650, Loss: 4.5123\n",
            "Step 1700, Loss: 4.3052\n",
            "Step 1750, Loss: 4.2526\n",
            "Step 1800, Loss: 4.2739\n",
            "Step 1850, Loss: 4.3151\n",
            "Step 1900, Loss: 4.2877\n",
            "Step 1950, Loss: 4.3339\n",
            "Step 2000, Loss: 4.2462\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 2050, Loss: 4.1577\n",
            "Step 2100, Loss: 4.1795\n",
            "Step 2150, Loss: 4.2001\n",
            "Step 2200, Loss: 4.2222\n",
            "Step 2250, Loss: 4.1918\n",
            "Step 2300, Loss: 4.1873\n",
            "Step 2350, Loss: 4.1406\n",
            "Step 2400, Loss: 4.1978\n",
            "Step 2450, Loss: 4.1479\n",
            "Step 2500, Loss: 4.0111\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 2550, Loss: 4.0808\n",
            "Step 2600, Loss: 4.0264\n",
            "Step 2650, Loss: 4.1336\n",
            "Step 2700, Loss: 4.0072\n",
            "Step 2750, Loss: 4.1272\n",
            "Step 2800, Loss: 4.0003\n",
            "Step 2850, Loss: 4.0978\n",
            "Step 2900, Loss: 3.9072\n",
            "Step 2950, Loss: 3.9082\n",
            "Step 3000, Loss: 3.8564\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 3050, Loss: 3.8950\n",
            "Step 3100, Loss: 3.9405\n",
            "Step 3150, Loss: 3.9764\n",
            "Step 3200, Loss: 3.9260\n",
            "Step 3250, Loss: 3.8506\n",
            "Step 3300, Loss: 3.9971\n",
            "Step 3350, Loss: 3.9023\n",
            "Step 3400, Loss: 3.7995\n",
            "Step 3450, Loss: 3.8860\n",
            "Step 3500, Loss: 3.8354\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 3550, Loss: 3.8108\n",
            "Step 3600, Loss: 3.8738\n",
            "Step 3650, Loss: 3.7432\n",
            "Step 3700, Loss: 3.8734\n",
            "Step 3750, Loss: 3.8835\n",
            "Step 3800, Loss: 3.8119\n",
            "Step 3850, Loss: 3.7399\n",
            "Step 3900, Loss: 3.8540\n",
            "Step 3950, Loss: 3.7385\n",
            "Step 4000, Loss: 3.8404\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 4050, Loss: 3.8057\n",
            "Step 4100, Loss: 3.6564\n",
            "Step 4150, Loss: 3.7826\n",
            "Step 4200, Loss: 3.6333\n",
            "Step 4250, Loss: 3.7592\n",
            "Step 4300, Loss: 3.6586\n",
            "Step 4350, Loss: 3.6515\n",
            "Step 4400, Loss: 3.7371\n",
            "Step 4450, Loss: 3.6235\n",
            "Step 4500, Loss: 3.7701\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 4550, Loss: 3.6929\n",
            "Step 4600, Loss: 3.6466\n",
            "Step 4650, Loss: 3.6755\n",
            "Step 4700, Loss: 3.6170\n",
            "Step 4750, Loss: 3.6519\n",
            "Step 4800, Loss: 3.5988\n",
            "Step 4850, Loss: 3.6175\n",
            "Step 4900, Loss: 3.6009\n",
            "Step 4950, Loss: 3.6272\n",
            "Step 5000, Loss: 3.6080\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 5050, Loss: 3.6837\n",
            "Step 5100, Loss: 3.6242\n",
            "Step 5150, Loss: 3.6742\n",
            "Step 5200, Loss: 3.6131\n",
            "Step 5250, Loss: 3.5285\n",
            "Step 5300, Loss: 3.6760\n",
            "Step 5350, Loss: 3.6585\n",
            "Step 5400, Loss: 3.5302\n",
            "Step 5450, Loss: 3.5658\n",
            "Step 5500, Loss: 3.6945\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 5550, Loss: 3.5890\n",
            "Step 5600, Loss: 3.6449\n",
            "Step 5650, Loss: 3.5029\n",
            "Step 5700, Loss: 3.5334\n",
            "Step 5750, Loss: 3.6197\n",
            "Step 5800, Loss: 3.5596\n",
            "Step 5850, Loss: 3.5252\n",
            "Step 5900, Loss: 3.6556\n",
            "Step 5950, Loss: 3.5551\n",
            "Step 6000, Loss: 3.4910\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 6050, Loss: 3.5619\n",
            "Step 6100, Loss: 3.5562\n",
            "Step 6150, Loss: 3.5538\n",
            "Step 6200, Loss: 3.5540\n",
            "Step 6250, Loss: 3.4170\n",
            "Step 6300, Loss: 3.5246\n",
            "Step 6350, Loss: 3.4857\n",
            "Step 6400, Loss: 3.5426\n",
            "Step 6450, Loss: 3.5729\n",
            "Step 6500, Loss: 3.5073\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 6550, Loss: 3.5356\n",
            "Step 6600, Loss: 3.5452\n",
            "Step 6650, Loss: 3.5180\n",
            "Step 6700, Loss: 3.5056\n",
            "Step 6750, Loss: 3.4520\n",
            "Step 6800, Loss: 3.4248\n",
            "Step 6850, Loss: 3.5835\n",
            "Step 6900, Loss: 3.4169\n",
            "Step 6950, Loss: 3.5144\n",
            "Step 7000, Loss: 3.4877\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 7050, Loss: 3.4699\n",
            "Step 7100, Loss: 3.4793\n",
            "Step 7150, Loss: 3.3978\n",
            "Step 7200, Loss: 3.3873\n",
            "Step 7250, Loss: 3.4892\n",
            "Step 7300, Loss: 3.4311\n",
            "Step 7350, Loss: 3.5004\n",
            "Step 7400, Loss: 3.3487\n",
            "Step 7450, Loss: 3.4337\n",
            "Step 7500, Loss: 3.4237\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 7550, Loss: 3.4822\n",
            "Step 7600, Loss: 3.5033\n",
            "Step 7650, Loss: 3.4746\n",
            "Step 7700, Loss: 3.4484\n",
            "Step 7750, Loss: 3.3746\n",
            "Step 7800, Loss: 3.4337\n",
            "Step 7850, Loss: 3.4069\n",
            "Step 7900, Loss: 3.4234\n",
            "Step 7950, Loss: 3.3655\n",
            "Step 8000, Loss: 3.3827\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 8050, Loss: 3.3783\n",
            "Step 8100, Loss: 3.5015\n",
            "Step 8150, Loss: 3.3538\n",
            "Step 8200, Loss: 3.4657\n",
            "Step 8250, Loss: 3.4658\n",
            "Step 8300, Loss: 3.4511\n",
            "Step 8350, Loss: 3.4328\n",
            "Step 8400, Loss: 3.3603\n",
            "Step 8450, Loss: 3.4360\n",
            "Step 8500, Loss: 3.3060\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 8550, Loss: 3.4127\n",
            "Step 8600, Loss: 3.3390\n",
            "Step 8650, Loss: 3.4220\n",
            "Step 8700, Loss: 3.3556\n",
            "Step 8750, Loss: 3.2348\n",
            "Step 8800, Loss: 3.3917\n",
            "Step 8850, Loss: 3.3674\n",
            "Step 8900, Loss: 3.2966\n",
            "Step 8950, Loss: 3.3778\n",
            "Step 9000, Loss: 3.2614\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 9050, Loss: 3.3391\n",
            "Step 9100, Loss: 3.2726\n",
            "Step 9150, Loss: 3.3058\n",
            "Step 9200, Loss: 3.3405\n",
            "Step 9250, Loss: 3.3309\n",
            "Step 9300, Loss: 3.3080\n",
            "Step 9350, Loss: 3.4528\n",
            "Step 9400, Loss: 3.3076\n",
            "Step 9450, Loss: 3.2554\n",
            "Step 9500, Loss: 3.3537\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 9550, Loss: 3.3590\n",
            "Step 9600, Loss: 3.3653\n",
            "Step 9650, Loss: 3.3608\n",
            "Step 9700, Loss: 3.3711\n",
            "Step 9750, Loss: 3.3189\n",
            "Step 9800, Loss: 3.3920\n",
            "Step 9850, Loss: 3.3509\n",
            "Step 9900, Loss: 3.2907\n",
            "Step 9950, Loss: 3.3961\n",
            "Step 10000, Loss: 3.2335\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 10050, Loss: 3.2709\n",
            "Step 10100, Loss: 3.2326\n",
            "Step 10150, Loss: 3.2699\n",
            "Step 10200, Loss: 3.2578\n",
            "Step 10250, Loss: 3.3232\n",
            "Step 10300, Loss: 3.3390\n",
            "Step 10350, Loss: 3.2895\n",
            "Step 10400, Loss: 3.3921\n",
            "Step 10450, Loss: 3.1889\n",
            "Step 10500, Loss: 3.3237\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 10550, Loss: 3.3469\n",
            "Step 10600, Loss: 3.2287\n",
            "Step 10650, Loss: 3.4184\n",
            "Step 10700, Loss: 3.3219\n",
            "Step 10750, Loss: 3.2663\n",
            "Step 10800, Loss: 3.3240\n",
            "Step 10850, Loss: 3.3261\n",
            "Step 10900, Loss: 3.3209\n",
            "Step 10950, Loss: 3.2475\n",
            "Step 11000, Loss: 3.3302\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 11050, Loss: 3.3543\n",
            "Step 11100, Loss: 3.2505\n",
            "Step 11150, Loss: 3.3017\n",
            "Step 11200, Loss: 3.2739\n",
            "Step 11250, Loss: 3.2672\n",
            "Step 11300, Loss: 3.2555\n",
            "Step 11350, Loss: 3.3028\n",
            "Step 11400, Loss: 3.2533\n",
            "Step 11450, Loss: 3.2785\n",
            "Step 11500, Loss: 3.1816\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 11550, Loss: 3.3583\n",
            "Step 11600, Loss: 3.1997\n",
            "Step 11650, Loss: 3.2392\n",
            "Step 11700, Loss: 3.3358\n",
            "Step 11750, Loss: 3.2151\n",
            "Step 11800, Loss: 3.0664\n",
            "Step 11850, Loss: 3.2391\n",
            "Step 11900, Loss: 3.2418\n",
            "Step 11950, Loss: 3.2342\n",
            "Step 12000, Loss: 3.2769\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 12050, Loss: 3.3771\n",
            "Step 12100, Loss: 3.2214\n",
            "Step 12150, Loss: 3.2297\n",
            "Step 12200, Loss: 3.2817\n",
            "Step 12250, Loss: 3.2413\n",
            "Step 12300, Loss: 3.2333\n",
            "Step 12350, Loss: 3.1552\n",
            "Step 12400, Loss: 3.2344\n",
            "Step 12450, Loss: 3.2304\n",
            "Step 12500, Loss: 3.3939\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 12550, Loss: 3.2321\n",
            "Step 12600, Loss: 3.3091\n",
            "Step 12650, Loss: 3.2369\n",
            "Step 12700, Loss: 3.2262\n",
            "Step 12750, Loss: 3.2986\n",
            "Step 12800, Loss: 3.2462\n",
            "Step 12850, Loss: 3.2056\n",
            "Step 12900, Loss: 3.2231\n",
            "Step 12950, Loss: 3.2477\n",
            "Step 13000, Loss: 3.1651\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 13050, Loss: 3.1931\n",
            "Step 13100, Loss: 3.1470\n",
            "Step 13150, Loss: 3.2706\n",
            "Step 13200, Loss: 3.1134\n",
            "Step 13250, Loss: 3.2222\n",
            "Step 13300, Loss: 3.2037\n",
            "Step 13350, Loss: 3.1805\n",
            "Step 13400, Loss: 3.1285\n",
            "Step 13450, Loss: 3.1852\n",
            "Step 13500, Loss: 3.2341\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 13550, Loss: 3.0740\n",
            "Step 13600, Loss: 3.1638\n",
            "Step 13650, Loss: 3.1839\n",
            "Step 13700, Loss: 3.1719\n",
            "Step 13750, Loss: 3.2788\n",
            "Step 13800, Loss: 3.2241\n",
            "Step 13850, Loss: 3.1245\n",
            "Step 13900, Loss: 3.1769\n",
            "Step 13950, Loss: 3.2064\n",
            "Step 14000, Loss: 3.1654\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 14050, Loss: 3.1708\n",
            "Step 14100, Loss: 3.1881\n",
            "Step 14150, Loss: 3.2642\n",
            "Step 14200, Loss: 3.1908\n",
            "Step 14250, Loss: 3.2267\n",
            "Step 14300, Loss: 3.0676\n",
            "Step 14350, Loss: 3.1700\n",
            "Step 14400, Loss: 3.3008\n",
            "Step 14450, Loss: 3.0936\n",
            "Step 14500, Loss: 3.2339\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 14550, Loss: 3.1294\n",
            "Step 14600, Loss: 3.1545\n",
            "Step 14650, Loss: 3.2010\n",
            "Step 14700, Loss: 3.1295\n",
            "Step 14750, Loss: 3.0314\n",
            "Step 14800, Loss: 3.1614\n",
            "Step 14850, Loss: 3.2615\n",
            "Step 14900, Loss: 3.1221\n",
            "Step 14950, Loss: 3.0951\n",
            "Step 15000, Loss: 3.1327\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 15050, Loss: 3.1540\n",
            "Step 15100, Loss: 3.0622\n",
            "Step 15150, Loss: 3.2065\n",
            "Step 15200, Loss: 3.1740\n",
            "Step 15250, Loss: 3.2065\n",
            "Step 15300, Loss: 3.1439\n",
            "Step 15350, Loss: 3.0706\n",
            "Step 15400, Loss: 3.2202\n",
            "Step 15450, Loss: 3.1775\n",
            "Step 15500, Loss: 3.1286\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 15550, Loss: 3.1226\n",
            "Step 15600, Loss: 3.1170\n",
            "Step 15650, Loss: 3.1976\n",
            "Step 15700, Loss: 3.1277\n",
            "Step 15750, Loss: 3.1929\n",
            "Step 15800, Loss: 3.0762\n",
            "Step 15850, Loss: 3.1297\n",
            "Step 15900, Loss: 3.0188\n",
            "Step 15950, Loss: 3.1325\n",
            "Step 16000, Loss: 3.1999\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 16050, Loss: 3.0611\n",
            "Step 16100, Loss: 3.1382\n",
            "Step 16150, Loss: 3.1335\n",
            "Step 16200, Loss: 3.1714\n",
            "Step 16250, Loss: 3.1356\n",
            "Step 16300, Loss: 3.2098\n",
            "Step 16350, Loss: 3.0380\n",
            "Step 16400, Loss: 3.1048\n",
            "Step 16450, Loss: 3.0703\n",
            "Step 16500, Loss: 3.0790\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 16550, Loss: 3.1895\n",
            "Step 16600, Loss: 3.1664\n",
            "Step 16650, Loss: 3.2644\n",
            "Step 16700, Loss: 3.0964\n",
            "Step 16750, Loss: 3.0546\n",
            "Step 16800, Loss: 3.2102\n",
            "Step 16850, Loss: 3.0096\n",
            "Step 16900, Loss: 3.1898\n",
            "Step 16950, Loss: 3.1332\n",
            "Step 17000, Loss: 3.1248\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 17050, Loss: 3.1144\n",
            "Step 17100, Loss: 3.1209\n",
            "Step 17150, Loss: 3.0649\n",
            "Step 17200, Loss: 3.0631\n",
            "Step 17250, Loss: 3.1220\n",
            "Step 17300, Loss: 3.1346\n",
            "Step 17350, Loss: 3.2120\n",
            "Step 17400, Loss: 2.9862\n",
            "Step 17450, Loss: 3.0385\n",
            "Step 17500, Loss: 3.0763\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 17550, Loss: 3.1592\n",
            "Step 17600, Loss: 3.0682\n",
            "Step 17650, Loss: 3.1573\n",
            "Step 17700, Loss: 3.1298\n",
            "Step 17750, Loss: 3.1734\n",
            "Step 17800, Loss: 3.1311\n",
            "Step 17850, Loss: 3.0873\n",
            "Step 17900, Loss: 3.1473\n",
            "Step 17950, Loss: 3.0088\n",
            "Step 18000, Loss: 3.1142\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 18050, Loss: 2.9931\n",
            "Step 18100, Loss: 3.0748\n",
            "Step 18150, Loss: 3.0728\n",
            "Step 18200, Loss: 3.0346\n",
            "Step 18250, Loss: 3.0442\n",
            "Step 18300, Loss: 3.1492\n",
            "Step 18350, Loss: 3.1064\n",
            "Step 18400, Loss: 2.9976\n",
            "Step 18450, Loss: 3.0724\n",
            "Step 18500, Loss: 3.1105\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 18550, Loss: 3.1007\n",
            "Step 18600, Loss: 3.1239\n",
            "Step 18650, Loss: 3.0215\n",
            "Step 18700, Loss: 3.0663\n",
            "Step 18750, Loss: 3.0357\n",
            "Step 18800, Loss: 3.0091\n",
            "Step 18850, Loss: 3.0526\n",
            "Step 18900, Loss: 3.0175\n",
            "Step 18950, Loss: 2.9472\n",
            "Step 19000, Loss: 3.0989\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 19050, Loss: 3.1125\n",
            "Step 19100, Loss: 2.9928\n",
            "Step 19150, Loss: 3.1566\n",
            "Step 19200, Loss: 3.1272\n",
            "Step 19250, Loss: 3.0801\n",
            "Step 19300, Loss: 3.0257\n",
            "Step 19350, Loss: 3.0182\n",
            "Step 19400, Loss: 3.0407\n",
            "Step 19450, Loss: 3.0854\n",
            "Step 19500, Loss: 3.0791\n",
            "Model saved to /content/drive/MyDrive/minigpt2.pth\n",
            "Step 19550, Loss: 3.1157\n",
            "Step 19600, Loss: 3.1160\n",
            "Step 19650, Loss: 3.0457\n",
            "Step 19700, Loss: 3.0162\n",
            "Step 19750, Loss: 3.0298\n",
            "Step 19800, Loss: 3.0000\n",
            "Step 19850, Loss: 3.0295\n",
            "Step 19900, Loss: 3.1372\n",
            "Step 19950, Loss: 3.0809\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
        "\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attention = CasualSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 256\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.2\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                token_embedding=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                position_embedding=nn.Embedding(config.block_size, config.n_embd),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        token_embeddings = self.transformer.token_embedding(idx)\n",
        "        position_ids = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        position_embeddings = self.transformer.position_embedding(position_ids)\n",
        "        x = token_embeddings + position_embeddings\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path, block_size):\n",
        "        self.data = np.memmap(path, dtype=np.uint16, mode=\"r\")\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx : idx + self.block_size + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "block_size = 256\n",
        "batch_size = 96\n",
        "train_steps = 20000\n",
        "save_path = \"/content/drive/MyDrive/minigpt2.pth\"\n",
        "\n",
        "\n",
        "train_dataset = GPTDataset(\"/content/drive/MyDrive/wikitext103_tokens.bin\", block_size)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4\n",
        ")\n",
        "\n",
        "\n",
        "config = GPTConfig(block_size=block_size)\n",
        "model = GPT(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "step = 0\n",
        "while step < train_steps:\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        if step >= train_steps:\n",
        "            break\n",
        "        model.train()\n",
        "        x_batch, y_batch = x_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits, loss = model(x_batch, y_batch)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if step % 500 == 0 and step > 0:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "        step += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}