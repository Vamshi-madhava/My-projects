{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "65GRU3RiNz4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAmaA6eYy1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab659bf-a93a-408f-d76b-463264450b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-4043524157.py:153: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-1-4043524157.py:165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 11.0159\n",
            "Step 50, Loss: 6.4785\n",
            "Step 100, Loss: 5.9868\n",
            "Step 150, Loss: 5.5971\n",
            "Step 200, Loss: 5.6113\n",
            "Step 250, Loss: 5.4783\n",
            "Step 300, Loss: 5.3896\n",
            "Step 350, Loss: 5.3517\n",
            "Step 400, Loss: 5.2054\n",
            "Step 450, Loss: 5.1116\n",
            "Step 500, Loss: 5.0215\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 550, Loss: 5.0513\n",
            "Step 600, Loss: 4.9194\n",
            "Step 650, Loss: 4.8346\n",
            "Step 700, Loss: 4.9784\n",
            "Step 750, Loss: 4.8763\n",
            "Step 800, Loss: 4.8115\n",
            "Step 850, Loss: 4.7562\n",
            "Step 900, Loss: 4.7052\n",
            "Step 950, Loss: 4.7728\n",
            "Step 1000, Loss: 4.7358\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 1050, Loss: 4.6686\n",
            "Step 1100, Loss: 4.6747\n",
            "Step 1150, Loss: 4.6122\n",
            "Step 1200, Loss: 4.6696\n",
            "Step 1250, Loss: 4.5487\n",
            "Step 1300, Loss: 4.5601\n",
            "Step 1350, Loss: 4.6381\n",
            "Step 1400, Loss: 4.5630\n",
            "Step 1450, Loss: 4.5715\n",
            "Step 1500, Loss: 4.5216\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 1550, Loss: 4.3642\n",
            "Step 1600, Loss: 4.4098\n",
            "Step 1650, Loss: 4.3822\n",
            "Step 1700, Loss: 4.4998\n",
            "Step 1750, Loss: 4.4212\n",
            "Step 1800, Loss: 4.3337\n",
            "Step 1850, Loss: 4.4068\n",
            "Step 1900, Loss: 4.4197\n",
            "Step 1950, Loss: 4.3505\n",
            "Step 2000, Loss: 4.3524\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 2050, Loss: 4.4029\n",
            "Step 2100, Loss: 4.2953\n",
            "Step 2150, Loss: 4.4266\n",
            "Step 2200, Loss: 4.2350\n",
            "Step 2250, Loss: 4.3031\n",
            "Step 2300, Loss: 4.2907\n",
            "Step 2350, Loss: 4.3628\n",
            "Step 2400, Loss: 4.2633\n",
            "Step 2450, Loss: 4.3174\n",
            "Step 2500, Loss: 4.2162\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 2550, Loss: 4.2351\n",
            "Step 2600, Loss: 4.3194\n",
            "Step 2650, Loss: 4.2175\n",
            "Step 2700, Loss: 4.1766\n",
            "Step 2750, Loss: 4.2332\n",
            "Step 2800, Loss: 4.1257\n",
            "Step 2850, Loss: 4.1087\n",
            "Step 2900, Loss: 4.2183\n",
            "Step 2950, Loss: 4.2132\n",
            "Step 3000, Loss: 4.0135\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 3050, Loss: 4.2345\n",
            "Step 3100, Loss: 4.0800\n",
            "Step 3150, Loss: 4.1143\n",
            "Step 3200, Loss: 4.1723\n",
            "Step 3250, Loss: 4.1295\n",
            "Step 3300, Loss: 4.1159\n",
            "Step 3350, Loss: 4.2514\n",
            "Step 3400, Loss: 4.1220\n",
            "Step 3450, Loss: 4.1575\n",
            "Step 3500, Loss: 4.1134\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 3550, Loss: 4.1732\n",
            "Step 3600, Loss: 4.1164\n",
            "Step 3650, Loss: 4.1229\n",
            "Step 3700, Loss: 3.9956\n",
            "Step 3750, Loss: 4.1819\n",
            "Step 3800, Loss: 4.1463\n",
            "Step 3850, Loss: 4.0101\n",
            "Step 3900, Loss: 4.0690\n",
            "Step 3950, Loss: 4.0850\n",
            "Step 4000, Loss: 4.1238\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 4050, Loss: 4.0054\n",
            "Step 4100, Loss: 4.0278\n",
            "Step 4150, Loss: 4.0102\n",
            "Step 4200, Loss: 4.0407\n",
            "Step 4250, Loss: 4.0059\n",
            "Step 4300, Loss: 4.0413\n",
            "Step 4350, Loss: 4.0985\n",
            "Step 4400, Loss: 4.0975\n",
            "Step 4450, Loss: 4.0517\n",
            "Step 4500, Loss: 3.9433\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 4550, Loss: 4.1008\n",
            "Step 4600, Loss: 4.0018\n",
            "Step 4650, Loss: 4.0014\n",
            "Step 4700, Loss: 4.0523\n",
            "Step 4750, Loss: 3.9902\n",
            "Step 4800, Loss: 3.9733\n",
            "Step 4850, Loss: 3.9581\n",
            "Step 4900, Loss: 4.0016\n",
            "Step 4950, Loss: 3.9965\n",
            "Step 5000, Loss: 4.0085\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 5050, Loss: 3.9677\n",
            "Step 5100, Loss: 4.0117\n",
            "Step 5150, Loss: 3.9939\n",
            "Step 5200, Loss: 3.9684\n",
            "Step 5250, Loss: 4.0822\n",
            "Step 5300, Loss: 3.9408\n",
            "Step 5350, Loss: 3.9739\n",
            "Step 5400, Loss: 3.9830\n",
            "Step 5450, Loss: 4.0014\n",
            "Step 5500, Loss: 3.9712\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 5550, Loss: 3.9604\n",
            "Step 5600, Loss: 3.9387\n",
            "Step 5650, Loss: 4.0843\n",
            "Step 5700, Loss: 3.8777\n",
            "Step 5750, Loss: 3.9756\n",
            "Step 5800, Loss: 3.9318\n",
            "Step 5850, Loss: 3.9989\n",
            "Step 5900, Loss: 4.0088\n",
            "Step 5950, Loss: 3.9090\n",
            "Step 6000, Loss: 3.9163\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 6050, Loss: 3.8917\n",
            "Step 6100, Loss: 3.9098\n",
            "Step 6150, Loss: 3.9241\n",
            "Step 6200, Loss: 3.8319\n",
            "Step 6250, Loss: 4.0117\n",
            "Step 6300, Loss: 3.8793\n",
            "Step 6350, Loss: 4.0057\n",
            "Step 6400, Loss: 3.8846\n",
            "Step 6450, Loss: 3.9035\n",
            "Step 6500, Loss: 3.8481\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 6550, Loss: 3.9622\n",
            "Step 6600, Loss: 3.8735\n",
            "Step 6650, Loss: 3.8909\n",
            "Step 6700, Loss: 3.8734\n",
            "Step 6750, Loss: 3.8815\n",
            "Step 6800, Loss: 3.8482\n",
            "Step 6850, Loss: 3.8617\n",
            "Step 6900, Loss: 3.7921\n",
            "Step 6950, Loss: 3.9462\n",
            "Step 7000, Loss: 3.9274\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 7050, Loss: 3.8943\n",
            "Step 7100, Loss: 3.9028\n",
            "Step 7150, Loss: 3.8751\n",
            "Step 7200, Loss: 3.9202\n",
            "Step 7250, Loss: 3.9527\n",
            "Step 7300, Loss: 3.9016\n",
            "Step 7350, Loss: 3.8509\n",
            "Step 7400, Loss: 3.8518\n",
            "Step 7450, Loss: 3.8518\n",
            "Step 7500, Loss: 3.8421\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 7550, Loss: 3.8316\n",
            "Step 7600, Loss: 3.8336\n",
            "Step 7650, Loss: 3.8301\n",
            "Step 7700, Loss: 3.8549\n",
            "Step 7750, Loss: 3.9149\n",
            "Step 7800, Loss: 3.8776\n",
            "Step 7850, Loss: 3.8772\n",
            "Step 7900, Loss: 3.7764\n",
            "Step 7950, Loss: 3.8870\n",
            "Step 8000, Loss: 4.0023\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 8050, Loss: 3.7513\n",
            "Step 8100, Loss: 3.8172\n",
            "Step 8150, Loss: 3.8460\n",
            "Step 8200, Loss: 3.8924\n",
            "Step 8250, Loss: 3.7837\n",
            "Step 8300, Loss: 3.7697\n",
            "Step 8350, Loss: 3.8165\n",
            "Step 8400, Loss: 3.7997\n",
            "Step 8450, Loss: 3.8210\n",
            "Step 8500, Loss: 3.8899\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 8550, Loss: 3.7781\n",
            "Step 8600, Loss: 3.8444\n",
            "Step 8650, Loss: 3.8311\n",
            "Step 8700, Loss: 3.8006\n",
            "Step 8750, Loss: 3.8873\n",
            "Step 8800, Loss: 3.8299\n",
            "Step 8850, Loss: 3.8511\n",
            "Step 8900, Loss: 3.8578\n",
            "Step 8950, Loss: 3.7765\n",
            "Step 9000, Loss: 3.8204\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 9050, Loss: 3.8015\n",
            "Step 9100, Loss: 3.7563\n",
            "Step 9150, Loss: 3.7548\n",
            "Step 9200, Loss: 3.7959\n",
            "Step 9250, Loss: 3.8402\n",
            "Step 9300, Loss: 3.8438\n",
            "Step 9350, Loss: 3.7618\n",
            "Step 9400, Loss: 3.6931\n",
            "Step 9450, Loss: 3.7624\n",
            "Step 9500, Loss: 3.8074\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 9550, Loss: 3.7957\n",
            "Step 9600, Loss: 3.6788\n",
            "Step 9650, Loss: 3.7404\n",
            "Step 9700, Loss: 3.7571\n",
            "Step 9750, Loss: 3.8499\n",
            "Step 9800, Loss: 3.7619\n",
            "Step 9850, Loss: 3.6550\n",
            "Step 9900, Loss: 3.7574\n",
            "Step 9950, Loss: 3.7344\n",
            "Step 10000, Loss: 3.7675\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 10050, Loss: 3.7165\n",
            "Step 10100, Loss: 3.7526\n",
            "Step 10150, Loss: 3.7423\n",
            "Step 10200, Loss: 3.7560\n",
            "Step 10250, Loss: 3.8027\n",
            "Step 10300, Loss: 3.7690\n",
            "Step 10350, Loss: 3.7769\n",
            "Step 10400, Loss: 3.7859\n",
            "Step 10450, Loss: 3.7517\n",
            "Step 10500, Loss: 3.6868\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 10550, Loss: 3.7453\n",
            "Step 10600, Loss: 3.7452\n",
            "Step 10650, Loss: 3.8011\n",
            "Step 10700, Loss: 3.7649\n",
            "Step 10750, Loss: 3.8126\n",
            "Step 10800, Loss: 3.7593\n",
            "Step 10850, Loss: 3.7415\n",
            "Step 10900, Loss: 3.7719\n",
            "Step 10950, Loss: 3.7610\n",
            "Step 11000, Loss: 3.7376\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 11050, Loss: 3.7378\n",
            "Step 11100, Loss: 3.8445\n",
            "Step 11150, Loss: 3.7437\n",
            "Step 11200, Loss: 3.7087\n",
            "Step 11250, Loss: 3.8153\n",
            "Step 11300, Loss: 3.6881\n",
            "Step 11350, Loss: 3.7620\n",
            "Step 11400, Loss: 3.6726\n",
            "Step 11450, Loss: 3.8083\n",
            "Step 11500, Loss: 3.7285\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 11550, Loss: 3.7370\n",
            "Step 11600, Loss: 3.6927\n",
            "Step 11650, Loss: 3.6402\n",
            "Step 11700, Loss: 3.7656\n",
            "Step 11750, Loss: 3.7317\n",
            "Step 11800, Loss: 3.7303\n",
            "Step 11850, Loss: 3.8254\n",
            "Step 11900, Loss: 3.6409\n",
            "Step 11950, Loss: 3.7559\n",
            "Step 12000, Loss: 3.7823\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 12050, Loss: 3.7159\n",
            "Step 12100, Loss: 3.8318\n",
            "Step 12150, Loss: 3.7223\n",
            "Step 12200, Loss: 3.7261\n",
            "Step 12250, Loss: 3.6754\n",
            "Step 12300, Loss: 3.7088\n",
            "Step 12350, Loss: 3.7745\n",
            "Step 12400, Loss: 3.6637\n",
            "Step 12450, Loss: 3.7167\n",
            "Step 12500, Loss: 3.7482\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 12550, Loss: 3.6742\n",
            "Step 12600, Loss: 3.6573\n",
            "Step 12650, Loss: 3.6954\n",
            "Step 12700, Loss: 3.7267\n",
            "Step 12750, Loss: 3.5619\n",
            "Step 12800, Loss: 3.8173\n",
            "Step 12850, Loss: 3.6793\n",
            "Step 12900, Loss: 3.7048\n",
            "Step 12950, Loss: 3.6489\n",
            "Step 13000, Loss: 3.6971\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 13050, Loss: 3.6968\n",
            "Step 13100, Loss: 3.7308\n",
            "Step 13150, Loss: 3.6985\n",
            "Step 13200, Loss: 3.7774\n",
            "Step 13250, Loss: 3.7749\n",
            "Step 13300, Loss: 3.7650\n",
            "Step 13350, Loss: 3.6533\n",
            "Step 13400, Loss: 3.6284\n",
            "Step 13450, Loss: 3.6601\n",
            "Step 13500, Loss: 3.5997\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 13550, Loss: 3.6936\n",
            "Step 13600, Loss: 3.6400\n",
            "Step 13650, Loss: 3.7331\n",
            "Step 13700, Loss: 3.6298\n",
            "Step 13750, Loss: 3.7025\n",
            "Step 13800, Loss: 3.7064\n",
            "Step 13850, Loss: 3.7567\n",
            "Step 13900, Loss: 3.6938\n",
            "Step 13950, Loss: 3.7541\n",
            "Step 14000, Loss: 3.7405\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 14050, Loss: 3.6710\n",
            "Step 14100, Loss: 3.6673\n",
            "Step 14150, Loss: 3.6830\n",
            "Step 14200, Loss: 3.7284\n",
            "Step 14250, Loss: 3.6859\n",
            "Step 14300, Loss: 3.7227\n",
            "Step 14350, Loss: 3.7065\n",
            "Step 14400, Loss: 3.6451\n",
            "Step 14450, Loss: 3.6756\n",
            "Step 14500, Loss: 3.7184\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 14550, Loss: 3.6769\n",
            "Step 14600, Loss: 3.5651\n",
            "Step 14650, Loss: 3.6879\n",
            "Step 14700, Loss: 3.7050\n",
            "Step 14750, Loss: 3.7066\n",
            "Step 14800, Loss: 3.6197\n",
            "Step 14850, Loss: 3.7134\n",
            "Step 14900, Loss: 3.6592\n",
            "Step 14950, Loss: 3.7200\n",
            "Step 15000, Loss: 3.7207\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 15050, Loss: 3.6173\n",
            "Step 15100, Loss: 3.6676\n",
            "Step 15150, Loss: 3.7395\n",
            "Step 15200, Loss: 3.6420\n",
            "Step 15250, Loss: 3.7008\n",
            "Step 15300, Loss: 3.7517\n",
            "Step 15350, Loss: 3.6812\n",
            "Step 15400, Loss: 3.7399\n",
            "Step 15450, Loss: 3.6341\n",
            "Step 15500, Loss: 3.6636\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 15550, Loss: 3.6382\n",
            "Step 15600, Loss: 3.6471\n",
            "Step 15650, Loss: 3.6352\n",
            "Step 15700, Loss: 3.6654\n",
            "Step 15750, Loss: 3.6920\n",
            "Step 15800, Loss: 3.6354\n",
            "Step 15850, Loss: 3.6524\n",
            "Step 15900, Loss: 3.6694\n",
            "Step 15950, Loss: 3.6735\n",
            "Step 16000, Loss: 3.6612\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 16050, Loss: 3.7420\n",
            "Step 16100, Loss: 3.7899\n",
            "Step 16150, Loss: 3.6680\n",
            "Step 16200, Loss: 3.6463\n",
            "Step 16250, Loss: 3.7061\n",
            "Step 16300, Loss: 3.6366\n",
            "Step 16350, Loss: 3.6583\n",
            "Step 16400, Loss: 3.6696\n",
            "Step 16450, Loss: 3.6053\n",
            "Step 16500, Loss: 3.6075\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 16550, Loss: 3.6424\n",
            "Step 16600, Loss: 3.6579\n",
            "Step 16650, Loss: 3.6939\n",
            "Step 16700, Loss: 3.7542\n",
            "Step 16750, Loss: 3.5768\n",
            "Step 16800, Loss: 3.6364\n",
            "Step 16850, Loss: 3.7495\n",
            "Step 16900, Loss: 3.5441\n",
            "Step 16950, Loss: 3.6435\n",
            "Step 17000, Loss: 3.7655\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 17050, Loss: 3.6088\n",
            "Step 17100, Loss: 3.6375\n",
            "Step 17150, Loss: 3.6356\n",
            "Step 17200, Loss: 3.6349\n",
            "Step 17250, Loss: 3.6399\n",
            "Step 17300, Loss: 3.5697\n",
            "Step 17350, Loss: 3.6535\n",
            "Step 17400, Loss: 3.7044\n",
            "Step 17450, Loss: 3.6943\n",
            "Step 17500, Loss: 3.6185\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 17550, Loss: 3.6323\n",
            "Step 17600, Loss: 3.6554\n",
            "Step 17650, Loss: 3.6722\n",
            "Step 17700, Loss: 3.6478\n",
            "Step 17750, Loss: 3.6377\n",
            "Step 17800, Loss: 3.6174\n",
            "Step 17850, Loss: 3.7092\n",
            "Step 17900, Loss: 3.7037\n",
            "Step 17950, Loss: 3.5932\n",
            "Step 18000, Loss: 3.6417\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 18050, Loss: 3.5903\n",
            "Step 18100, Loss: 3.6732\n",
            "Step 18150, Loss: 3.5738\n",
            "Step 18200, Loss: 3.5430\n",
            "Step 18250, Loss: 3.8027\n",
            "Step 18300, Loss: 3.6072\n",
            "Step 18350, Loss: 3.5206\n",
            "Step 18400, Loss: 3.7057\n",
            "Step 18450, Loss: 3.6423\n",
            "Step 18500, Loss: 3.6992\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 18550, Loss: 3.6144\n",
            "Step 18600, Loss: 3.6597\n",
            "Step 18650, Loss: 3.6269\n",
            "Step 18700, Loss: 3.6180\n",
            "Step 18750, Loss: 3.6099\n",
            "Step 18800, Loss: 3.6815\n",
            "Step 18850, Loss: 3.6490\n",
            "Step 18900, Loss: 3.5560\n",
            "Step 18950, Loss: 3.6639\n",
            "Step 19000, Loss: 3.5655\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 19050, Loss: 3.6757\n",
            "Step 19100, Loss: 3.7058\n",
            "Step 19150, Loss: 3.6742\n",
            "Step 19200, Loss: 3.6584\n",
            "Step 19250, Loss: 3.6480\n",
            "Step 19300, Loss: 3.5711\n",
            "Step 19350, Loss: 3.6053\n",
            "Step 19400, Loss: 3.6825\n",
            "Step 19450, Loss: 3.5910\n",
            "Step 19500, Loss: 3.6046\n",
            "Model saved to /content/drive/MyDrive/gpt8x512.pth\n",
            "Step 19550, Loss: 3.6741\n",
            "Step 19600, Loss: 3.6955\n",
            "Step 19650, Loss: 3.6180\n",
            "Step 19700, Loss: 3.6001\n",
            "Step 19750, Loss: 3.6430\n",
            "Step 19800, Loss: 3.6187\n",
            "Step 19850, Loss: 3.6914\n",
            "Step 19900, Loss: 3.6982\n",
            "Step 19950, Loss: 3.6406\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
        "\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attention = CasualSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 128\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 8\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 512\n",
        "    dropout: float = 0.2\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                token_embedding=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                position_embedding=nn.Embedding(config.block_size, config.n_embd),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        token_embeddings = self.transformer.token_embedding(idx)\n",
        "        position_ids = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        position_embeddings = self.transformer.position_embedding(position_ids)\n",
        "        x = token_embeddings + position_embeddings\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "class GPTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path, block_size):\n",
        "        self.data = np.memmap(path, dtype=np.uint16, mode=\"r\")\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx : idx + self.block_size + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "block_size = 128\n",
        "batch_size = 96\n",
        "train_steps = 20000\n",
        "save_path = \"/content/drive/MyDrive/gpt8x512.pth\"\n",
        "\n",
        "\n",
        "train_dataset = GPTDataset(\"/content/drive/MyDrive/bookcorpus_tokens.bin\", block_size)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4\n",
        ")\n",
        "\n",
        "\n",
        "config = GPTConfig(block_size=block_size)\n",
        "model = GPT(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "step = 0\n",
        "while step < train_steps:\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        if step >= train_steps:\n",
        "            break\n",
        "        model.train()\n",
        "        x_batch, y_batch = x_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits, loss = model(x_batch, y_batch)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if step % 500 == 0 and step > 0:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "        step += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}